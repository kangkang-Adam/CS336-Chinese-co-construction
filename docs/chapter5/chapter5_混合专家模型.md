# 第五章 专家混合模型
专家混合模型（MoE）是当前LLM领域中一项至关重要的技术，它有效地解决了模型规模与计算成本之间的矛盾。这种机制允许模型在不显著增加训练和推理计算量的前提下，大幅扩展其总参数规模和表达能力，从而实现了模型容量（参数数量）与计算效率之间的动态平衡。正是凭借MoE这一机制，Switch Transformer、DeepSeek等模型得以问世并展现出卓越性能。然而，MoE在实际应用中会有负载均衡、跨设备并行、训练不稳定和路由机制设计等工程挑战。接下来，我们将剖析MoE的核心概念、工作原理以及实际应用，并提供解决这些工程挑战的实用思路，旨在以更高的计算效率，正确应用这一机制来扩展LLM的能力。

## 5.1 分析MoE
混合专家模型通过将原本的单一前馈网络（如MLP、FFN）替换为由多个并行子网络组成的专家集合，并通过路由机制在每次计算中仅激活少数专家，从而在保持单次前向计算量（FLOPs）基本不变的前提下显著提升模型的参数容量与表达能力。其核心思想是：模型总体包含大规模参数，但每个输入只使用其中一小部分专家，使得**容量大但计算稀疏**。

`路由机制`指在一次前向传播中，从所有专家中选择出少量最适合当前输入的专家共同参与计算，通常也被称为门控机制。假设一共有 $N$ 个专家，输入为 $x$ ，门控函数为 $G(\cdot)$ ，用于决定每个专家的权重， $E_i(\cdot)$ 表示第 $i$ 个专家的输出。则门控机制的核心计算公式为：

$$
y = \sum_{i \in \mathcal{T}} G_i(x) E_i(x)
$$

**关键点在于集合 $\mathcal{T}$：**
稀疏化的关键步骤，这里的 $\mathcal{T}$ 是通过 **$Top-k$** 机制选出的索引集合。通常 $G(x)$ 的计算过程包含两个步骤：
1.  **打分：** 计算路由分数 $h(x) = x \cdot W_g$。
2.  **稀疏化：** 仅保留分数最高的 $k$ 个专家（激活 $k$ 个专家），并进行Softmax归一化。

>注意：在稀疏化的步骤中，未被选中的 $N-k$ 个专家的路由权重被强制置为零，这意味着这绝大多数专家在本次前向传播中完全不参与计算，从而保证了FLOPs的稀疏性。

因此，实际计算只发生在被激活的专家 $i \in \mathcal{T}$ 上实现了计算的稀疏化，*总结，门控机制 = 选拔+对选中的专家进行加权*。

### 5.1.1 概念直观理解

### 5.1.2 MoE与Transformer
唯有一处关键差异在自注意力层的前馈网络（FFN）。

<div align="center">
   <img width="2000" height="600" alt="image" src="https://github.com/user-attachments/assets/d1480f2b-2957-407d-b26e-ad18ddacd779" />
   <p>图2.1 稠密与混合专家模型</p>
 </div>
 
在稠密模型中，前馈网络是单一模块；而在稀疏模型即混合专家模型中，这个FFN进行拆分或复制，这取决于具体任务。根据MoE稀疏激活的工作方式，架构中会拥有多个全连接网络的副本，并通过路由器在每次前向传播或推理时选择其中的少量几个专家。这就是MoE的基本理念——用选择器层和多个小型前馈网络替代左侧的单一大型前馈模块。这种架构的优势何在？由于稀疏激活特性（比如仅选择一个专家），且每个专家规模与稠密FFN相同，那么左右两侧的稠密模型与MoE模型具有相同的FLOPs——它们在执行前向传播时进行着相同的矩阵运算。这意味着你可以在不影响FLOPs的情况下获得更多参数。如果你认同更多参数有助于记忆世界知识等任务，那么这无疑是绝佳的架构设计。现在你应该能直观理解MoE的优势了。
### 5.1.3 数学与概率视角
### 5.1.4 基本变体
## 5.2 MoE与LLM

  ### 5.2.1 路由算法与负载均衡
  ### 5.2.2 训练不稳定性
  ### 5.2.3 推理与部署
## 5.3 DeepSeek创新与实战复现

  ### 5.3.1 DeepSeek的创新关键点
  ### 5.3.2 小规模复现实验
## 5.4 近期MoE研究

## 思考
## 参考文献

