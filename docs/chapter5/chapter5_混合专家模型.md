# 第五章 混合专家机制
## 5.1 分析MoE

### 5.1.1 MoE理论

### 5.1.2 MoE运用

## 5.2 MoE与LLM

### 5.2.1 MoE运用在LLM的架构方式

### 5.2.2 MoE的限制

## 5.3 DeepSeek创新使用MoE

### 5.3.1 DeepSeek创新解决点

### 5.3.2 尝试复现DeepSeek解决思路用于小规模语言模型

## 5.4 近期对于MoE的研究

## 思考

# 参考文献

## 5.1 分析MoE

  ### 5.1.1 概念直观
  ### 5.1.2 数学与概率视角（gating 函数、top-k、noisy-topk、capacity factor 的定义与公式推导）
  ### 5.1.3 基本变体
## 5.2 MoE与LLM

  ### 5.2.1 在 Transformer 中的插入点（FFN 替换、多个 MoE 层、局部/全局路由）
  ### 5.2.2 路由算法与负载均衡
  ### 5.2.3 训练不稳定性与常见修复
  ### 5.2.4 推理与部署
## 5.3 DeepSeek创新与实战复现（保留但更具体）

  ### 5.3.1 DeepSeek的创新关键点
  ### 5.3.2 小规模复现实验：实验设计、配置、baseline 与评价指标
## 5.4 近期研究与未来方向（保留），同时加入“争议与局限”小节
## 思考
## 参考文献

