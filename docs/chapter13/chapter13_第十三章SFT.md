# 第十三章：SFT（Supervised Fine-Tuning）监督微调

经过之前的章节，我们基本掌握了模型的结构，pytorch和如何推理等等，在这节课我们重点介绍一下模型的训练过程。主要介绍大语言模型（LLM）的训练过程，在这个过程中我们会介绍预训练、监督微调、强化学习方法等，主要讲解模型SFT的过程，简单介绍预训练和强化学习方法。本节课会穿插扩展2025年cs336没有的内容，在下一章详细介绍强化学习方法。

## 13.1. 机器学习的常见的学习方式

### 13.1.1有监督学习（Supervised Learning）

有监督学习是机器学习中**最常用、最直接**的一种范式：

给定一组**输入–输出成对**的标注样本 $(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)$
目标是让模型学会一个映射函数 $f: x \mapsto y$ 
使得对新输入 $x_{\text{new}}$ 能尽可能准确地预测出对应的输出 $y_{\text{new}}$。

**特点**

1. **有“标准答案”**：每个样本的输出 \(y\) 都是人工或可靠系统事先标注好的。  
2. **损失可计算**：预测值 \(\hat{y}\) 与真值 \(y\) 之间的误差（交叉熵、MSE 等）可直接作为优化信号。  
3. **目标明确**：最小化训练集上的预测误差，同时兼顾泛化能力（防止过拟合）。

**典型任务**常见的有**分类任务**（离散标签）比如图像识别：图片 → “猫/狗/车”，**回归任务**（连续值） 房价预测：房屋特征 → 价格，等等，有监督学习就是“老师把答案写在卷子上”——模型通过对比自己的答案和标准答案不断纠正错误（通过一些算法，比如梯度下降），从而学会对新题目给出正确结果。它的数据集是有**标准答案**的，标准答案就是一个**监督信号**，所以叫做有监督学习。

---

### 13.1.2 无监督学习（Unsupervised Learning）

**没有“标准答案”、只有“原材料”即原始数据**，算法的目标不是预测某个具体标签，而是**从数据本身里挖掘出隐藏的结构或分布特征**。

与有监督学习相比，有监督学习的输入为$(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)$ ，它是有标签的，比如说实现一个猫图识别，判断图片是否是猫，是则输出1，不是则输出0，零和一就是标签。输入的 $y$ 就是标签。而无监督学习是没有标签的

**只给输入 x，不给输出 y**，让机器自己找规律、找相似、找低维表示或生成新样本。常见的无监督任务就有**聚类（Clustering）** 把相似样本自动分到一组。  无监督学习就是**不给答案、只给卷子**，让机器自己把题目分堆、找规律、画重点，甚至还能照猫画虎出一份新卷子。

**自监督学习**（Self-supervised Learning）是无监督学习的一个子集，它**把“没有标签”的原始数据自己生成伪标签**，然后再按“有监督”的方式去训练。因此它既属于无监督大家庭，又带上了“假装有监督”的味道。比如说大模型的预训练，bert的Masked Language Modeling，还有对比学习等等

无监督学习的优点就是它**不需要标注，无需昂贵标注，数据拿来就能用；常作为预训练或探索工具**。在机器学习或者说深度学习中有标签的数据一直都是一个难题，他们往往都需要昂贵的人工标注，但是无监督学习能帮工程师省下标注的费用，当然不是所有的任务都能使用无监督的，还得回到具体，实事求是。

---  

### 13.1.4 强化学习（Reinforcement Learning）

强化学习比较复杂，我们会在下一章中详细介绍。强化学习是用延迟、稀疏的奖励信号，让智能体在序列决策中通过试错+价值估计，自己摸索出长期最赚的行动策略。如果说有监督学习是老师给每题标准答案，无监督学习就是没老师，自己找结构和规律，强化学习就是老师只期末给总评（奖励），学生全程自己摸索哪一步对哪一步错。

---

## 13.2 大模型训练的第一个阶段：预训练（Pre-training，PT）

第一个明确采用“预训练 + 下游微调”这一范式的大型语言模型是 2018 年 OpenAI 发布的 GPT-1。它首次把“无监督预训练 → 有监督微调”的路线系统化：先在 5 GB 的 BooksCorpus 上用**自回归语言模型目标**做大规模无监督预训练，再在少量标注数据上微调具体任务，从而显著超越当时只能从头训练的模型。

**大模型的预训练**（Pre-training）是让模型 先在海量无标注数据上“自学”通用知识，得到一个强大的底座，然后再用少量标注数据去微调解决具体任务。它本质上是迁移学习的极端放大版：把“从数据里学通用表示”这一步做到极致。


当时语言模型没有预训练的范式，每个模型训练都要耗费大量时间和人力去获取训练数据，当时已经出现了预训练 + 特定任务微调的范式，但是是在图像任务上--ImageNet,先在ImageNet上训练分类网络，再在增加**小批量标注数据**继续训练。只需要**少量数据**就训练出一个很好的模型，只需要训练**一个预训练模型**就可以在各种下游任务轻松应用。

### 13.2.1 大模型预训练的范式

大模型通常都是decoder-only的结构，大模型的预训练的范式是不断的预测下一个词。是next-token预测。最终训练得到一个续写模型，能根据输入来不断续写，此时大模型已经通过预训练获取到了许多先验的知识。

模型的输入（input）和标签（label）它们共同用于训练模型以预测下一个词或字符。



- **目标序列**：对于输入序列 $[x_1, x_2, \dots, x_{t-1}]$，目标（标签）是序列中的下一个词 $x_t$。
- **训练目标**：模型的目标是学习如何根据输入序列 $[x_1, x_2, \dots, x_{t-1}]$ 准确地预测出下一个词 $x_t$ 的概率分布 $P(x_t | x_1, x_2, \dots, x_{t-1})$。

假设我们有一个文本序列：

```markdown
"自然语言处理是人工智能的一个重要分支"
```

我们将这个序列分词成长度为4的子序列（假设）进行训练。

输入和标签的对应关系：

- **输入序列**：`["自然", "语言", "处理"]`
  - **标签**：`"是"`
- **输入序列**：`["语言", "处理", "是"]`
  - **标签**：`"人工智能"`
- **输入序列**：`["处理", "是", "人工智能"]`
  - **标签**：`"的一个"`
- **输入序列**：`["是", "人工智能", "的一个"]`
  - **标签**：`"重要分支"`

这就是next-token的预训练范式。

---

### 13.2.2 大模型的预训练的数据规模

大模型的预训练数据通过抓取公开网页、书籍、论文、代码、多语种语料，再进行去重、数据清洗得到，训练词表。一个8B的模型比如Qwen3-8B就会用到36T tokens，**更大的模型只会随着参数量，数据的规模变得更大**。现在的大模型大概会用到50–200 T tokens。

大模型预训练的数据**基本包含人类的所有知识**，因此模型蕴含的知识是十分丰富的，但是模型现在只是一个**续写模型**，你给出一段文本，模型就会进行续写，因为它是不断预测下一个字来训练的。想要更好的利用模型的能力还需要一个SFT的流程才能得到今天问答形式的大模型，可以处理各种任务。

虽然预训练规模巨大，但模型**不能很好地遵循指令**，缺乏产品化价值。预训练模型需要经过特定的训练后处理，才能变得实用和安全。

我们期望模型能够**遵循复杂指令**，具备实用性。同时增强**模型的安全性**，防止滥用和生成有害内容。

---

### 13.2.3 GPT3（Generative Pre-trained Transformer 3）

GPT-3（Generative Pre-trained Transformer 3）是 OpenAI 于 2020 年 7 月发布的 **自回归语言模型**，它的出现把“提示即编程”带进现实，被视为大模型时代的里程碑。GPT-3 用 **1750 亿参数 + 自回归语言模型 + 纯提示** 第一次证明：  **“只要够大，模型就能在没有任何梯度更新的情况下，看懂任务并给出像模像样的答案。”**，为后来的 InstructGPT、ChatGPT、GPT-4 铺平道路。

GPT-3 在骨子里就是一个 **“续写”模型**——它的训练目标只有 **“给定上文，预测下一个 token”**（自回归语言模型）。无论把提示写成问答、翻译、对话还是代码补全，它都当成 **“前面这段话还没完，我来续写”** 来处理。

GPT3是一个175B参数量的模型，在大约570GB的文本上进行训练。它是头一个把参数量设置设置非常大的模型，这是一个非常大胆的尝试。**规模定律第一次“肉眼可见”**  从 GPT-2 的 1.5 B 跳到 175 B，参数 ×100，结果下游任务出现 **“涌现”**——只靠提示就能解决翻译、问答、算术、代码补全等任务。

| 任务 | 指标 | 成绩 |
|---|---|---|
| 英文阅读理解（RACE） | 准确率 | 86.8 %，超越人类平均 73 % |
| 翻译（WMT’14 法→英） | BLEU | 43.9 ，接近当时最佳有监督系统 |
| 数学加法（2～5 位数） | 准确率 | 随示例数从 0 % 提升到 80 % |
| 代码补全（HumanEval） | 通过率 | 37 %（Codex 继续微调后提到 72 %） |

- **幻觉严重**：一本正经地编新闻、假引用。  
- **偏见大**：性别、种族、宗教刻板印象随提示直接输出。  

**那如何使用GPT3**

GPT3是一个预训练大模型，是一个具备续写能力的模型，要使用它要比现在的模型麻烦的多。

首先要将输入改动用户把任务包装成自然语言上下文，例如： 

```
Translate English to French:
sea otter →
```  

形式上像“翻译”，本质仍是“补全”。

## 13.3大模型训练的第二个阶段：监督微调（SFT，Supervised Fine-Tuning）


### 13.3.1 SFT的定义与作用：

通过专家演示数据对预训练模型进行微调，使其能够模仿SFT数据中的行为。它也是构建指令遵循模型的第一步。SFT是监督微调，通过预训练大模型已经掌握了通用的知识，同时通过大规模的预训练，我们避免了大规模的数据标注，只需要一些远比预训练数据集**小的多**的SFT数据集（10 k～100 k），这其实就是预训练的意义之一。SFT数据通常都是问答的形式，Q...，A....的形式，通过交叉熵损失等等损失函数来训练，使得模型学到SFT数据的格式，增加了模型的可用性。

**预训练底座模型**有许多**缺点**：

1. 只会“续写”，不会“问答” 
2. 可能会输出**有害或者偏见**内容
3. 答案散漫，跑题，**有严重的幻觉，也不会角色扮演**、工具调用。

我们**期待的模型**长什么样子呢？

1. 学会指令格式，**QA的形式来使用模型**，比如让模型写一篇鲁迅风格的文章，李白风格的诗。我们说一，模型就不会回答二。
2. **会拒绝有害内容**，当用户使用大模型生成一些有害内容的时候，大模型会学会拒绝。
3. 学会SFT中的回答格式，学会工具调用。

这些都可以通过SFT实现。

---

### 13.3.2 SFT数据的格式

SFT（Supervised Fine-Tuning）数据的核心是“给模型看**人写的标准回答**”，让它模仿。主流格式就两类：

1. **Alpaca 格式（单轮/指令）**

每行一条 JSON，字段一目了然：

```json
{
  "instruction": "翻译成英文",
  "input": "你好",
  "output": "Hello"
}
```

- `instruction` 说明任务  
- `input` 放用户问题（可为空）  
- `output` 是**人工写的理想答案**  

文件整体是 `.jsonl`：一行一条，训练时只计算 `output` 部分的交叉熵损失 。

---

2. **ChatML / ShareGPT 格式（多轮对话）**

把多轮对话按角色堆成数组，同样一行一条：

```json
{
  "messages": [
    {"role": "system", "content": "你是客服助手"},
    {"role": "user", "content": "怎么修改收货地址？"},
    {"role": "assistant", "content": "请在订单详情页点击…"}
  ]
}
```

它支持任意轮次，训练时只对 **assistant** 角色的 token 计算损失 。

SFT 数据就是“**问题 + 人类示范答案**”的配对，单轮用 Alpaca，多轮用 ChatML，格式简单，关键是答案要干净、安全、风格一致。

---

### 13.3.3 高质量的专家演示数据对SFT效果至关重要

有许多论文都论证高质量SFT数据的重要性，SFT阶段和预训练阶段不同，预训练需要庞大的数据，数据越多越好，在这种思维惯性下，你可能会任务SFT数据也是越多越好，忽略的质量的重要性。数据量虽少，但能显著塑造模型行为。

许多论文都提到这种现象：MergeIT（arXiv2503.00034）用小模型筛出6k高质量指令，再与全量模型做权重插值，最终 LLaMA-7B 仅用1/11数据就在 AlpacaEval 上追平65k全量训练，From Quantity to Quality（arXiv:2308.12032，已被 NAACL 2024 接收）做了实验用 9 k 精选样本即可在 5 个公开benchmark上持续优于原始50k全量训练的同款模型，且消融实验完整，代码与数据均已开源。

李飞飞团队的在2025年发表的S1论文提到：用**1000条**蒸馏自 Gemini-2.0-Flash-Thinking 的高质量推理样本（s1K）对Qwen2.5-32B-Instruct做26分钟监督微调（16×H100），再配合“预算强制” 解码策略，即可在 AIME24 等数学基准上与 OpenAI-o1-preview 打平甚至略超，训练云成本 ≈ 50 美元，他们从16个数学/科学题库收集59k题，**难度/多样性/质量三重过滤蒸馏**Gemini思维链，最终s1K 仅1000样本。而且是纯纯监督微调，证明了1k高质量示范>几十k普通标注，呼应“Less is More”趋势。

**为什么会这样**？传统的思维告诉我们一个经验：**越多越好，或者量变产生质变**，这种经验在这里就失去了作用。

**核心原因**是：机器学习模型的参数完全由**数据驱动**，“学什么”决定“会什么”。劣质数据（**错误标签、噪声、缺失、偏见**）不会被模型忘记，反而会被参数记忆，导致性能下降、泛化变差、健壮性不足。

有一篇论文[《The Effects of Data Quality on Machine Learning Performance》](https://ar5iv.labs.arxiv.org/html/2207.14529)专门介绍了这件事情：

他们9个公开表格数据集，15 种经典算法（逻辑回归、SVM、DT、KNN、MLP 等）使用6类污染机制——Target Accuracy（标签错误）、Feature Accuracy（特征噪声）、Completeness（缺失值）、Uniqueness（重复样本）、Consistent Representation（取值不一致）、Class Balance（类别不平衡）来污染逐步污染数据来测试，

**核心结论（对应小语言模型/经典算法场景）**

1. **标签错误（Target Accuracy）（最直接影响）**
   - 训练集标签每**翻转 1%**，F1 分数**线性下降约 2–5%**；  
   - 当翻转率 ≥ 20% 时，多数分类器性能**低于多数类 baseline**（即“学不如猜”）。

2. **特征噪声（Feature Accuracy）**
   - 同样呈现**线性衰减**；  
   - 在小型数据集（Credit, 1 000 条）上，MLP 与 SVM 方差显著增大，**对噪声最敏感**。

3. **缺失值（Completeness）**
   - 若训练阶段**从未见过缺失值**，而测试时出现 20% 缺失，F1 可掉 10% 以上；  
   - 若在训练阶段就引入 ≤40% 缺失，模型可学会“容忍”，性能下降不显著。

4. **重复样本（Uniqueness）**
   - 在万级样本以上的数据集，**去重与否几乎不影响准确率**；  
   - 但在**小样本**（<1 k）场景，5% 重复就能让决策树/MLP 显著过拟合，**F1 掉 4–6%**，因此仍需去重。

5. **类别不平衡（Class Balance）**
   - 只要**少数类样本数 ≥ 1/类别数**，分类器仍能保持高于 baseline 的表现；  
   - 一旦少数类被“稀释”到 <1/类别数，所有算法性能**迅速滑向 majority baseline**。

这个团队一共做了**15 算法 × 5 折 × 6 质量维度 × 3 场景 = 4 050 组实验**，论文的结论是**标签准确率 ≥ 80%** 即可接受，不必追求 100% 人工重标；**测试集必须人工二次校验**，否则 40% 标签错误会把“好模型”误判为不如 majority baseline；**小数据集一定要先去重、再训练**；大数据集可跳过去重步骤，把预算投入标签修正。

在经典小模型场景，该论文用4050组实验证实：**标签错误 1% → 性能线性掉 2–5%；>20% 错误率直接让模型“学不如猜”**，为**数据质量 > 数据数量**提供了可量化的统计学证据。

- **数据集的构建方式**：
    - **FLAN数据集**：聚合多个自然语言处理任务的数据集，如问答、主题分类等。
    - **OpenAssistant数据集**：由线上爱好者共同编写，包含大量人工撰写的指令和回复。
    - **斯坦福Alpaca项目**：使用语言模型生成指令调优数据，通过人类编写的指令种子集扩展数据。

- **数据的特点与挑战**：
    - **FLAN数据集**：任务形式化，回复简短，可能不够自然。
    - **OpenAssistant数据集**：回复详细，质量高，但构建难度大。
    - **Alpaca数据**：生成速度快，但可能存在多样性不足的问题。

- **数据质量与模型行为**：
    - 数据中的噪声会导致模型行为异常。
    - 回复长度、风格、事实准确性等都会影响模型输出。

- **安全性与数据**：
    - 需要平衡模型的拒绝回答能力，避免过度拒绝或生成有害内容。
    - 即使是少量的安全调优数据，也能显著提升模型安全性。
