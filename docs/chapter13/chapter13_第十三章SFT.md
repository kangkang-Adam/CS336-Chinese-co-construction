# 第十三章：SFT（Supervised Fine-Tuning）监督微调

经过之前的章节，我们基本掌握了模型的结构，pytorch和如何推理等等，在这节课我们重点介绍一下模型的训练过程。主要介绍大语言模型（LLM）的训练过程，在这个过程中我们会介绍预训练、监督微调、强化学习方法等，主要讲解模型SFT的过程，简单介绍预训练和强化学习方法。本节课会穿插扩展2025年cs336没有的内容，在下一章详细介绍强化学习方法。

## 13.1. 机器学习的常见的学习方式

### 13.1.1有监督学习（Supervised Learning）

有监督学习是机器学习中**最常用、最直接**的一种范式：

给定一组**输入–输出成对**的标注样本 $(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)$
目标是让模型学会一个映射函数 $f: x \mapsto y$ 
使得对新输入 $x_{\text{new}}$ 能尽可能准确地预测出对应的输出 $y_{\text{new}}$。

**特点**

1. **有“标准答案”**：每个样本的输出 \(y\) 都是人工或可靠系统事先标注好的。  
2. **损失可计算**：预测值 \(\hat{y}\) 与真值 \(y\) 之间的误差（交叉熵、MSE 等）可直接作为优化信号。  
3. **目标明确**：最小化训练集上的预测误差，同时兼顾泛化能力（防止过拟合）。

**典型任务**常见的有**分类任务**（离散标签）比如图像识别：图片 → “猫/狗/车”，**回归任务**（连续值） 房价预测：房屋特征 → 价格，等等，有监督学习就是“老师把答案写在卷子上”——模型通过对比自己的答案和标准答案不断纠正错误（通过一些算法，比如梯度下降），从而学会对新题目给出正确结果。它的数据集是有**标准答案**的，标准答案就是一个**监督信号**，所以叫做有监督学习。

---

### 13.1.2 无监督学习（Unsupervised Learning）

**没有“标准答案”、只有“原材料”即原始数据**，算法的目标不是预测某个具体标签，而是**从数据本身里挖掘出隐藏的结构或分布特征**。

与有监督学习相比，有监督学习的输入为$(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)$ ，它是有标签的，比如说实现一个猫图识别，判断图片是否是猫，是则输出1，不是则输出0，零和一就是标签。输入的 $y$ 就是标签。而无监督学习是没有标签的

**只给输入 x，不给输出 y**，让机器自己找规律、找相似、找低维表示或生成新样本。常见的无监督任务就有**聚类（Clustering）** 把相似样本自动分到一组。  无监督学习就是**不给答案、只给卷子**，让机器自己把题目分堆、找规律、画重点，甚至还能照猫画虎出一份新卷子。

**自监督学习**（Self-supervised Learning）是无监督学习的一个子集，它**把“没有标签”的原始数据自己生成伪标签**，然后再按“有监督”的方式去训练。因此它既属于无监督大家庭，又带上了“假装有监督”的味道。比如说大模型的预训练，bert的Masked Language Modeling，还有对比学习等等

无监督学习的优点就是它**不需要标注，无需昂贵标注，数据拿来就能用；常作为预训练或探索工具**。在机器学习或者说深度学习中有标签的数据一直都是一个难题，他们往往都需要昂贵的人工标注，但是无监督学习能帮工程师省下标注的费用，当然不是所有的任务都能使用无监督的，还得回到具体，实事求是。

---  

### 13.1.4 强化学习（Reinforcement Learning）

强化学习比较复杂，我们会在下一章中详细介绍。强化学习是用延迟、稀疏的奖励信号，让智能体在序列决策中通过试错+价值估计，自己摸索出长期最赚的行动策略。如果说有监督学习是老师给每题标准答案，无监督学习就是没老师，自己找结构和规律，强化学习就是老师只期末给总评（奖励），学生全程自己摸索哪一步对哪一步错。

---

## 13.2 大模型训练的第一个阶段：预训练（Pre-training，PT）

第一个明确采用“预训练 + 下游微调”这一范式的大型语言模型是 2018 年 OpenAI 发布的 GPT-1。它首次把“无监督预训练 → 有监督微调”的路线系统化：先在 5 GB 的 BooksCorpus 上用自回归语言模型目标做大规模无监督预训练，再在少量标注数据上微调具体任务，从而显著超越当时只能从头训练的模型。

大模型的预训练（Pre-training）是让模型 先在海量无标注数据上“自学”通用知识，得到一个强大的底座，然后再用少量标注数据去微调解决具体任务。它本质上是迁移学习的极端放大版：把“从数据里学通用表示”这一步做到极致。


当时语言模型没有预训练的范式，每个模型训练都要耗费大量时间和人力去获取训练数据，当时已经出现了预训练 + 特定任务微调的范式，但是是在图像任务上--ImageNet,先在ImageNet上训练分类网络，再在增加小批量标注数据继续训练。只需要少量数据就训练出一个很好的模型，只需要训练一个预训练模型就可以在各种下游任务轻松应用。

### 13.2.1 大模型预训练的范式

大模型通常都是decoder-only的结构，大模型的预训练的范式是不断的预测下一个词。是next-token预测。最终训练得到一个续写模型，能根据输入来不断续写，此时大模型已经通过预训练获取到了许多先验的知识。

模型的输入（input）和标签（label）它们共同用于训练模型以预测下一个词或字符。



- **目标序列**：对于输入序列 $[x_1, x_2, \dots, x_{t-1}]$，目标（标签）是序列中的下一个词 $x_t$。
- **训练目标**：模型的目标是学习如何根据输入序列 $[x_1, x_2, \dots, x_{t-1}]$ 准确地预测出下一个词 $x_t$ 的概率分布 $P(x_t | x_1, x_2, \dots, x_{t-1})$。

假设我们有一个文本序列：

```markdown
"自然语言处理是人工智能的一个重要分支"
```

我们将这个序列分割成长度为4的子序列进行训练。

输入和标签的对应关系：

- **输入序列**：`["自然", "语言", "处理"]`
  - **标签**：`"是"`
- **输入序列**：`["语言", "处理", "是"]`
  - **标签**：`"人工智能"`
- **输入序列**：`["处理", "是", "人工智能"]`
  - **标签**：`"的一个"`
- **输入序列**：`["是", "人工智能", "的一个"]`
  - **标签**：`"重要分支"`

这就是next-token的预训练范式。

### 13.2.2 大模型的预训练的数据规模

大模型的预训练数据通过抓取公开网页、书籍、论文、代码、多语种语料，再进行去重、数据清洗得到，训练词表。一个8B的模型比如Qwen3-8B就会用到36T tokens，更大的模型只会随着参数量，数据的规模变得更大。现在的大模型大概会用到50–200 T tokens。

大模型预训练的数据基本包含人类的所有知识，因此模型蕴含的知识是十分丰富的，但是模型现在只是一个续写模型，你给出一段文本，模型就会进行续写，因为它是不断预测下一个字来训练的。想要更好的利用模型的能力还需要一个SFT的流程才能得到今天问答形式的大模型，可以处理各种任务。

虽然预训练规模巨大，但模型不能很好地遵循指令，缺乏产品化价值。预训练模型需要经过特定的训练后处理，才能变得实用和安全。

我们期望模型能够遵循复杂指令，具备实用性。同时增强模型的安全性，防止滥用和生成有害内容。

## 13.3大模型的监督微调（SFT，Supervised Fine-Tuning）


### 13.3.1**SFT的定义与作用**：

通过专家演示数据对预训练模型进行微调，使其能够模仿SFT数据中的行为。它也是构建指令遵循模型的第一步。SFT是监督微调，通过预训练大模型已经掌握了通用的知识，同时通过大规模的预训练，我们避免了大规模的数据标注，只需要一些远比预训练数据集**小的多**的SFT数据集（10 k～100 k），这其实就是预训练的意义之一。SFT数据通常都是问答的形式，Q...，A....的形式，通过交叉熵损失等等损失函数来训练，使得模型学到SFT数据的格式，增加了模型的可用性。

**预训练底座模型**有许多**缺点**：

1. 只会“续写”，不会“问答” 
2. 可能会输出**有害或者偏见**内容
3. 答案散漫，跑题，**有严重的幻觉，也不会角色扮演**、工具调用。

我们**期待的模型**长什么样子呢？

1. 学会指令格式，QA的形式来使用模型，比如让模型写一篇鲁迅风格的文章，李白风格的诗。我们说一，模型就不会回答而。
2. 会拒绝有害内容，当用户使用大模型生成一些有害内容的时候，大模型会学会拒绝。
3. 学会SFT中的回答格式，学会工具调用。

这些都可以通过SFT

- **数据的重要性**：
    - 高质量的专家演示数据对SFT效果至关重要。
    - 数据量虽少，但能显著塑造模型行为。

- **数据集的构建方式**：
    - **FLAN数据集**：聚合多个自然语言处理任务的数据集，如问答、主题分类等。
    - **OpenAssistant数据集**：由线上爱好者共同编写，包含大量人工撰写的指令和回复。
    - **斯坦福Alpaca项目**：使用语言模型生成指令调优数据，通过人类编写的指令种子集扩展数据。

- **数据的特点与挑战**：
    - **FLAN数据集**：任务形式化，回复简短，可能不够自然。
    - **OpenAssistant数据集**：回复详细，质量高，但构建难度大。
    - **Alpaca数据**：生成速度快，但可能存在多样性不足的问题。

- **数据质量与模型行为**：
    - 数据中的噪声会导致模型行为异常。
    - 回复长度、风格、事实准确性等都会影响模型输出。

- **安全性与数据**：
    - 需要平衡模型的拒绝回答能力，避免过度拒绝或生成有害内容。
    - 即使是少量的安全调优数据，也能显著提升模型安全性。
